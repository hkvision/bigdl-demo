{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "friesian_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORdnlMNukrEerSe8cE/rMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hkvision/bigdl-demo/blob/main/friesian_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Preparation"
      ],
      "metadata": {
        "id": "KeSWNiiV9p9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3toUa5UfCc0u",
        "outputId": "8c18de7e-185b-43f6-ccc8-18241767b401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n",
            "openjdk version \"1.8.0_342\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_342-8u342-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.342-b07, mixed mode)\n"
          ]
        }
      ],
      "source": [
        "# Install jdk8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "# Set environment variable JAVA_HOME.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --pre --upgrade bigdl-friesian-spark3[train]\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fg35k0L_G6Pu",
        "outputId": "a7aaf0d6-3157-4a75-a3ac-2c8626646ce7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bigdl-friesian-spark3[train]\n",
            "  Downloading bigdl_friesian_spark3-2.1.0b20220820-py3-none-manylinux1_x86_64.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting bigdl-orca-spark3==2.1.0b20220820\n",
            "  Downloading bigdl_orca_spark3-2.1.0b20220820-py3-none-manylinux1_x86_64.whl (21.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.8 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting bigdl-tf==0.14.0.dev1\n",
            "  Downloading bigdl_tf-0.14.0.dev1-py3-none-manylinux2010_x86_64.whl (71.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 71.0 MB 327 bytes/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (21.3)\n",
            "Collecting bigdl-dllib-spark3==2.1.0b20220820\n",
            "  Downloading bigdl_dllib_spark3-2.1.0b20220820-py3-none-manylinux1_x86_64.whl (50.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 50.0 MB 1.9 MB/s \n",
            "\u001b[?25hCollecting bigdl-math==0.14.0.dev1\n",
            "  Downloading bigdl_math-0.14.0.dev1-py3-none-manylinux2010_x86_64.whl (35.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 35.4 MB 190 kB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (3.8.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (23.2.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from bigdl-dllib-spark3==2.1.0b20220820->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.21.6)\n",
            "Collecting bigdl-core==2.1.0b20220820\n",
            "  Downloading bigdl_core-2.1.0b20220820-py3-none-manylinux2010_x86_64.whl (48.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.2 MB 1.8 MB/s \n",
            "\u001b[?25hCollecting pyspark==3.1.2\n",
            "  Downloading pyspark-3.1.2.tar.gz (212.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 64 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from bigdl-dllib-spark3==2.1.0b20220820->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.15.0)\n",
            "Collecting conda-pack==0.3.1\n",
            "  Downloading conda_pack-0.3.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from conda-pack==0.3.1->bigdl-dllib-spark3==2.1.0b20220820->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (57.4.0)\n",
            "Collecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 48.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout==4.0.1\n",
            "  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n",
            "Collecting aioredis==1.3.1\n",
            "  Downloading aioredis-1.3.1-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.6 MB/s \n",
            "\u001b[?25hCollecting hiredis==2.0.0\n",
            "  Downloading hiredis-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (85 kB)\n",
            "\u001b[K     |████████████████████████████████| 85 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (5.4.8)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting ray[default]==1.9.2\n",
            "  Downloading ray-1.9.2-cp37-cp37m-manylinux2014_x86_64.whl (57.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 57.6 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp==3.8.1 in /usr/local/lib/python3.7/dist-packages (from bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (3.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (22.1.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.3.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (6.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (4.1.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (2.1.0)\n",
            "Requirement already satisfied: protobuf>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (3.17.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (7.1.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (4.3.3)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.4.0rc1-py3-none-any.whl (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (6.0)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.47.0)\n",
            "Collecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.4.0_dev2-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 41.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (2.23.0)\n",
            "Collecting prometheus-client>=0.7.1\n",
            "  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.2 MB/s \n",
            "\u001b[?25hCollecting opencensus\n",
            "  Downloading opencensus-0.11.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 57.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.7/dist-packages (from ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (5.2.1)\n",
            "Collecting gpustat>=1.0.0b1\n",
            "  Downloading gpustat-1.0.0rc1.tar.gz (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting colorful\n",
            "  Downloading colorful-0.6.0a1-py2.py3-none-any.whl (202 kB)\n",
            "\u001b[K     |████████████████████████████████| 202 kB 50.8 MB/s \n",
            "\u001b[?25hCollecting nvidia-ml-py<=11.495.46,>=11.450.129\n",
            "  Downloading nvidia_ml_py-11.495.46-py3-none-any.whl (25 kB)\n",
            "Collecting psutil\n",
            "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
            "\u001b[K     |████████████████████████████████| 281 kB 44.1 MB/s \n",
            "\u001b[?25hCollecting blessed>=1.17.1\n",
            "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (0.2.5)\n",
            "Collecting deprecated>=1.2.3\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.5.0->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (4.12.0)\n",
            "Collecting redis>=3.5.0\n",
            "  Downloading redis-4.3.4-py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 50.4 MB/s \n",
            "\u001b[?25h  Downloading redis-4.3.3-py3-none-any.whl (244 kB)\n",
            "\u001b[K     |████████████████████████████████| 244 kB 51.1 MB/s \n",
            "\u001b[?25h  Downloading redis-4.3.2-py3-none-any.whl (244 kB)\n",
            "\u001b[K     |████████████████████████████████| 244 kB 51.8 MB/s \n",
            "\u001b[?25h  Downloading redis-4.3.1-py3-none-any.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 52.2 MB/s \n",
            "\u001b[?25h  Downloading redis-4.3.0-py3-none-any.whl (241 kB)\n",
            "\u001b[K     |████████████████████████████████| 241 kB 49.8 MB/s \n",
            "\u001b[?25h  Downloading redis-4.2.2-py3-none-any.whl (226 kB)\n",
            "\u001b[K     |████████████████████████████████| 226 kB 50.4 MB/s \n",
            "\u001b[?25h  Downloading redis-4.2.1-py3-none-any.whl (225 kB)\n",
            "\u001b[K     |████████████████████████████████| 225 kB 52.1 MB/s \n",
            "\u001b[?25h  Downloading redis-4.2.0-py3-none-any.whl (225 kB)\n",
            "\u001b[K     |████████████████████████████████| 225 kB 31.3 MB/s \n",
            "\u001b[?25h  Downloading redis-4.2.0rc3-py3-none-any.whl (224 kB)\n",
            "\u001b[K     |████████████████████████████████| 224 kB 62.3 MB/s \n",
            "\u001b[?25h  Downloading redis-4.2.0rc2-py3-none-any.whl (223 kB)\n",
            "\u001b[K     |████████████████████████████████| 223 kB 48.9 MB/s \n",
            "\u001b[?25h  Downloading redis-4.2.0rc1-py3-none-any.whl (220 kB)\n",
            "\u001b[K     |████████████████████████████████| 220 kB 49.0 MB/s \n",
            "\u001b[?25h  Downloading redis-4.1.4-py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.3->redis>=3.5.0->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.14.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.5.0->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (3.0.9)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.8.1->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (2.10)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (5.9.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (0.18.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.31.6)\n",
            "Collecting opencensus-context>=0.1.3\n",
            "  Downloading opencensus_context-0.2.dev0-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.56.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (2022.2.1)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.35.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (4.2.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]==1.9.2->bigdl-orca-spark3==2.1.0b20220820->bigdl-friesian-spark3[train]) (2022.6.15)\n",
            "Building wheels for collected packages: pyspark, gpustat\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880769 sha256=9d3399a9a40c1c9927903f40a1ac3112cb25131e5a2514d93abd8adbf575f829\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/0a/c1/9561f6fecb759579a7d863dcd846daaa95f598744e71b02c77\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.0.0rc1-py3-none-any.whl size=18872 sha256=d13deec33c7f13fb13e9c37265e548c6c05eb4acf158751dea0c6561f4045f33\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/85/03/7f87ed3a11307c5ad083829b59731788971a8411c265984409\n",
            "Successfully built pyspark gpustat\n",
            "Installing collected packages: py4j, deprecated, async-timeout, redis, pyspark, psutil, opencensus-context, nvidia-ml-py, hiredis, conda-pack, blessed, bigdl-core, ray, py-spy, prometheus-client, opencensus, gpustat, colorful, bigdl-tf, bigdl-math, bigdl-dllib-spark3, aioredis, aiohttp-cors, setproctitle, bigdl-orca-spark3, bigdl-friesian-spark3\n",
            "  Attempting uninstall: async-timeout\n",
            "    Found existing installation: async-timeout 4.0.2\n",
            "    Uninstalling async-timeout-4.0.2:\n",
            "      Successfully uninstalled async-timeout-4.0.2\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "Successfully installed aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-4.0.1 bigdl-core-2.1.0b20220820 bigdl-dllib-spark3-2.1.0b20220820 bigdl-friesian-spark3-2.1.0b20220820 bigdl-math-0.14.0.dev1 bigdl-orca-spark3-2.1.0b20220820 bigdl-tf-0.14.0.dev1 blessed-1.19.1 colorful-0.6.0a1 conda-pack-0.3.1 deprecated-1.2.13 gpustat-1.0.0rc1 hiredis-2.0.0 nvidia-ml-py-11.495.46 opencensus-0.11.0 opencensus-context-0.2.dev0 prometheus-client-0.14.1 psutil-5.9.1 py-spy-0.4.0.dev2 py4j-0.10.9 pyspark-3.1.2 ray-1.9.2 redis-4.1.4 setproctitle-1.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.26.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.47.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate random data for 2021 Twitter Recsys Challenge"
      ],
      "metadata": {
        "id": "Lt3c07B993VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType, BooleanType\n",
        "from bigdl.orca import init_orca_context, stop_orca_context, OrcaContext\n",
        "from bigdl.friesian.feature import FeatureTable\n",
        "\n",
        "# To display terminal's stdout and stderr in the Jupyter notebook.\n",
        "OrcaContext.log_output = True\n",
        "\n",
        "sc = init_orca_context(cores=4, init_ray_on_spark=True)\n",
        "spark = OrcaContext.get_spark_session()"
      ],
      "metadata": {
        "id": "BjyLvgS0ISio",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403d938f-85b1-4613-9016-a52a6d543e7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing orca context\n",
            "Current pyspark location is : /usr/local/lib/python3.7/dist-packages/pyspark/__init__.py\n",
            "Start to getOrCreate SparkContext\n",
            "pyspark_submit_args is:  --driver-class-path /usr/local/lib/python3.7/dist-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_3.1.2-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/usr/local/lib/python3.7/dist-packages/bigdl/share/orca/lib/bigdl-orca-spark_3.1.2-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/usr/local/lib/python3.7/dist-packages/bigdl/share/friesian/lib/bigdl-friesian-spark_3.1.2-2.1.0-SNAPSHOT-jar-with-dependencies.jar:/usr/local/lib/python3.7/dist-packages/bigdl/share/core/lib/all-2.1.0-20220728.053003-14.jar pyspark-shell \n",
            "Successfully got a SparkContext\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-22 10:10:46,981\tINFO services.py:1340 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://172.28.0.2:8265\u001b[39m\u001b[22m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'node_ip_address': '172.28.0.2', 'raylet_ip_address': '172.28.0.2', 'redis_address': '172.28.0.2:6379', 'object_store_address': '/tmp/ray/session_2022-08-22_10-10-40_148040_58/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-08-22_10-10-40_148040_58/sockets/raylet', 'webui_url': '172.28.0.2:8265', 'session_dir': '/tmp/ray/session_2022-08-22_10-10-40_148040_58', 'metrics_export_port': 48661, 'node_id': '13783e8fa862e962b39841bf7b49311d234fcb33c43ebaeba082cd88'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_list = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\",\n",
        "           \"9\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\",\n",
        "           \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\",\n",
        "           \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
        "media_list = [\"Photo\", \"Video\", \"GIF\"]\n",
        "tweet_list = [\"Retweet\", \"Quote\", \"TopLevel\"]\n",
        "language_list = [\"\".join(random.choices(id_list, k=32)) for _ in range(65)]"
      ],
      "metadata": {
        "id": "pEGXGI1xivRn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = StructType(\n",
        "    [StructField(\"text_tokens\", StringType(), True),\n",
        "     StructField(\"hashtags\", StringType(), True),\n",
        "     StructField(\"tweet_id\", StringType(), True),\n",
        "     StructField(\"present_media\", StringType(), True),\n",
        "     StructField(\"present_links\", StringType(), True),\n",
        "     StructField(\"present_domains\", StringType(), True),\n",
        "     StructField(\"tweet_type\", StringType(), True),\n",
        "     StructField(\"language\", StringType(), True),\n",
        "     StructField(\"tweet_timestamp\", LongType(), True),\n",
        "     StructField(\"engaged_with_user_id\", StringType(), True),\n",
        "     StructField(\"engaged_with_user_follower_count\", LongType(), True),\n",
        "     StructField(\"engaged_with_user_following_count\", LongType(), True),\n",
        "     StructField(\"engaged_with_user_is_verified\", BooleanType(), True),\n",
        "     StructField(\"engaged_with_user_account_creation\", LongType(), True),\n",
        "     StructField(\"enaging_user_id\", StringType(), True),\n",
        "     StructField(\"enaging_user_follower_count\", LongType(), True),\n",
        "     StructField(\"enaging_user_following_count\", LongType(), True),\n",
        "     StructField(\"enaging_user_is_verified\", BooleanType(), True),\n",
        "     StructField(\"enaging_user_account_creation\", LongType(), True),\n",
        "     StructField(\"engagee_follows_engager\", StringType(), True),\n",
        "     StructField(\"reply_timestamp\", LongType(), True),\n",
        "     StructField(\"retweet_timestamp\", LongType(), True),\n",
        "     StructField(\"retweet_with_comment_timestamp\", LongType(), True),\n",
        "     StructField(\"like_timestamp\", LongType(), True)])"
      ],
      "metadata": {
        "id": "lhQZsu6BixGw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_record(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    text_tokens = \"\\t\".join([str(random.randint(1, 1000))\n",
        "                            for i in range(random.randint(1, 10))])\n",
        "    hashtags = \"\\t\".join([\"\".join(random.choices(id_list, k=32))\n",
        "                          for i in range(random.randint(0, 50))])\n",
        "    tweet_id = \"\".join(random.choices(id_list, k=32))\n",
        "    present_media = \"\\t\".join(random.choices(\n",
        "        media_list, k=random.randint(0, 9)))\n",
        "    present_links = \"\\t\".join([\"\".join(random.choices(id_list, k=32))\n",
        "                               for i in range(random.randint(0, 10))])\n",
        "    present_domains = \"\\t\".join([\"\".join(random.choices(id_list, k=32))\n",
        "                                for i in range(random.randint(0, 10))])\n",
        "    tweet_type = random.choices(tweet_list)[0]\n",
        "    language = random.choices(language_list)[0]\n",
        "    tweet_timestamp = random.randint(946656000, 1609430400)\n",
        "    engaged_with_user_id = \"\".join(random.choices(id_list, k=32))\n",
        "    engaged_with_user_follower_count = random.randint(0, 10000)\n",
        "    engaged_with_user_following_count = random.randint(0, 10000)\n",
        "    engaged_with_user_is_verified = bool(random.getrandbits(1))\n",
        "    engaged_with_user_account_creation = random.randint(946656000, 1609430400)\n",
        "    enaging_user_id = \"\".join(random.choices(id_list, k=32))\n",
        "    enaging_user_follower_count = random.randint(0, 10000)\n",
        "    enaging_user_following_count = random.randint(0, 10000)\n",
        "    enaging_user_is_verified = bool(random.getrandbits(1))\n",
        "    enaging_user_account_creation = random.randint(946656000, 1609430400)\n",
        "    engagee_follows_engager = bool(random.getrandbits(1))\n",
        "    reply = bool(random.getrandbits(1))\n",
        "    reply_timestamp = random.randint(946656000, 1609430400) if reply else None\n",
        "    retweet = bool(random.getrandbits(1))\n",
        "    retweet_timestamp = random.randint(\n",
        "        946656000, 1609430400) if retweet else None\n",
        "    comment = bool(random.getrandbits(1))\n",
        "    retweet_with_comment_timestamp = random.randint(\n",
        "        946656000, 1609430400) if comment else None\n",
        "    like = bool(random.getrandbits(1))\n",
        "    like_timestamp = random.randint(946656000, 1609430400) if like else None\n",
        "    return (text_tokens, hashtags, tweet_id, present_media, present_links, present_domains,\n",
        "            tweet_type, language, tweet_timestamp, engaged_with_user_id,\n",
        "            engaged_with_user_follower_count, engaged_with_user_following_count,\n",
        "            engaged_with_user_is_verified, engaged_with_user_account_creation,\n",
        "            enaging_user_id, enaging_user_follower_count, enaging_user_following_count,\n",
        "            enaging_user_is_verified, enaging_user_account_creation,\n",
        "            engagee_follows_engager, reply_timestamp, retweet_timestamp,\n",
        "            retweet_with_comment_timestamp, like_timestamp)"
      ],
      "metadata": {
        "id": "NXcjHj61jDr-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = sc.parallelize(range(50000))\n",
        "dummy_data_rdd = rdd.map(generate_record)\n",
        "df = FeatureTable(spark.createDataFrame(dummy_data_rdd, schema))"
      ],
      "metadata": {
        "id": "00ak8wmqqaay"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tbl, valid_tbl = df.random_split([0.8, 0.2])\n",
        "\n",
        "train_size = train_tbl.size()\n",
        "valid_size = valid_tbl.size()\n",
        "print(\"Total number of train records: {}\".format(train_size))\n",
        "print(\"Total number of validation records: {}\".format(valid_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO9IjWwJr-WP",
        "outputId": "8ec1aff6-f144-4010-be86-95d343ee2594"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of train records: 40044\n",
            "Total number of validation records: 9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "zu6OkQrN97gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bool_cols = [\n",
        "    'engaged_with_user_is_verified',\n",
        "    'enaging_user_is_verified'\n",
        "]\n",
        "\n",
        "count_cols = [\n",
        "    'engaged_with_user_follower_count',\n",
        "    'engaged_with_user_following_count',\n",
        "    'enaging_user_follower_count',\n",
        "    'enaging_user_following_count'\n",
        "]\n",
        "\n",
        "cat_cols = [\n",
        "    'present_media',\n",
        "    'tweet_type',\n",
        "    'language'\n",
        "]"
      ],
      "metadata": {
        "id": "gICYuvr8uv-Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "media_map = {\n",
        "    '': 0,\n",
        "    'GIF': 1,\n",
        "    'GIF_GIF': 2,\n",
        "    'GIF_Photo': 3,\n",
        "    'GIF_Video': 4,\n",
        "    'Photo': 5,\n",
        "    'Photo_GIF': 6,\n",
        "    'Photo_Photo': 7,\n",
        "    'Photo_Video': 8,\n",
        "    'Video': 9,\n",
        "    'Video_GIF': 10,\n",
        "    'Video_Photo': 11,\n",
        "    'Video_Video': 12\n",
        "}\n",
        "\n",
        "type_map = {\n",
        "    'Quote': 0,\n",
        "    'Retweet': 1,\n",
        "    'TopLevel': 2,\n",
        "}"
      ],
      "metadata": {
        "id": "RkHadUUPuzeA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(tbl):\n",
        "    tbl = tbl.fillna(\"\", \"present_media\")\n",
        "    tbl = tbl.cast(bool_cols + count_cols, \"int\")  # cast bool and long to int\n",
        "    tbl = tbl.cut_bins(columns=count_cols,\n",
        "                       bins=[1, 1e2, 1e3, 1e4, 1e5, 1e6, 1e7],\n",
        "                       out_cols=count_cols)\n",
        "    process_media = lambda x: '_'.join(x.split('\\t')[:2])\n",
        "    tbl = tbl.apply(\"present_media\", \"present_media\", process_media, \"string\")\n",
        "    tbl = tbl.encode_string(\"present_media\", media_map)\n",
        "    tbl = tbl.encode_string(\"tweet_type\", type_map)\n",
        "\n",
        "    return tbl\n",
        "\n",
        "\n",
        "train_tbl = preprocess(train_tbl)\n",
        "valid_tbl = preprocess(valid_tbl)"
      ],
      "metadata": {
        "id": "yt5pEiR5u3ip"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tbl, language_idx = train_tbl.category_encode(\"language\")\n",
        "valid_tbl = valid_tbl.encode_string(\"language\", language_idx)\n",
        "valid_tbl = valid_tbl.fillna(0, \"language\")\n",
        "\n",
        "print(\"The number of languages: {}\".format(language_idx.size()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIm7OGcOu5EK",
        "outputId": "c6e19559-f347-4277-fbf5-c54c3ffc180f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-08-22 10:12:36,810\tWARNING worker.py:1245 -- (ip=172.28.0.2) The agent on node 7aa2ed812b32 failed to be restarted 5 times. There are 3 possible problems if you see this error.\n",
            "  1. The dashboard might not display correct information on this node.\n",
            "  2. Metrics on this node won't be reported.\n",
            "  3. runtime_env APIs won't work.\n",
            "Check out the `dashboard_agent.log` to see the detailed failure messages.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of languages: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_features(tbl):\n",
        "    cross_cols = [['present_media', 'language']]\n",
        "    cross_dims = [600]\n",
        "    tbl = tbl.cross_columns(cross_cols, cross_dims)  # The resulting cross column will have name \"present_media_language\"\n",
        "\n",
        "    count_func = lambda x: str(x).count('\\t') + 1 if x else 0\n",
        "    tbl = tbl.apply(\"hashtags\", \"len_hashtags\", count_func, \"int\") \\\n",
        "        .apply(\"present_domains\", \"len_domains\", count_func, \"int\") \\\n",
        "        .apply(\"present_links\", \"len_links\", count_func, \"int\")\n",
        "    return tbl\n",
        "\n",
        "\n",
        "train_tbl = generate_features(train_tbl)\n",
        "valid_tbl = generate_features(valid_tbl)"
      ],
      "metadata": {
        "id": "LMtzBZ5lvDtD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_cols = ['len_hashtags',\n",
        "            'len_domains',\n",
        "            'len_links']\n",
        "\n",
        "train_tbl, min_max_dict = train_tbl.min_max_scale(len_cols)\n",
        "valid_tbl = valid_tbl.transform_min_max_scale(len_cols, min_max_dict)"
      ],
      "metadata": {
        "id": "erSwEMs5vNpY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp_cols = [\n",
        "    'reply_timestamp',\n",
        "    'retweet_timestamp',\n",
        "    'retweet_with_comment_timestamp',\n",
        "    'like_timestamp'\n",
        "]"
      ],
      "metadata": {
        "id": "KXJAXemIvPRo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_label(tbl):\n",
        "    tbl = tbl.cast(timestamp_cols, \"int\")\n",
        "    tbl = tbl.fillna(0, timestamp_cols)\n",
        "    gen_label = lambda x: 1 if max(x) > 0 else 0\n",
        "    tbl = tbl.apply(in_col=timestamp_cols, out_col=\"label\", func=gen_label, dtype=\"int\")\n",
        "    return tbl\n",
        "\n",
        "\n",
        "train_tbl = transform_label(train_tbl)\n",
        "valid_tbl = transform_label(valid_tbl)"
      ],
      "metadata": {
        "id": "YbKOZft2vQl0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tbl.select(bool_cols + cat_cols).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFUSGTIOwAEK",
        "outputId": "d3ccbed0-a3f7-4b14-f0b3-2fcb02f0f1fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+------------------------+-------------+----------+--------+\n",
            "|engaged_with_user_is_verified|enaging_user_is_verified|present_media|tweet_type|language|\n",
            "+-----------------------------+------------------------+-------------+----------+--------+\n",
            "|                            0|                       0|           11|         1|      29|\n",
            "|                            0|                       1|           12|         1|      11|\n",
            "|                            1|                       0|            3|         0|       4|\n",
            "|                            1|                       1|            8|         1|      25|\n",
            "|                            0|                       1|            5|         2|       1|\n",
            "+-----------------------------+------------------------+-------------+----------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tbl.select(count_cols).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UOesaxxwFcJ",
        "outputId": "e7299e86-4886-4597-92cd-d2d9879534ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------+---------------------------------+---------------------------+----------------------------+\n",
            "|engaged_with_user_follower_count|engaged_with_user_following_count|enaging_user_follower_count|enaging_user_following_count|\n",
            "+--------------------------------+---------------------------------+---------------------------+----------------------------+\n",
            "|                               3|                                3|                          3|                           3|\n",
            "|                               3|                                3|                          3|                           3|\n",
            "|                               3|                                3|                          3|                           3|\n",
            "|                               3|                                3|                          3|                           2|\n",
            "|                               3|                                3|                          3|                           3|\n",
            "+--------------------------------+---------------------------------+---------------------------+----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tbl.select(len_cols + [\"present_media_language\", \"label\"]).show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DspJCJWwQn5",
        "outputId": "cbc15ff7-1b5f-434e-cc15-aafdd34d9091"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+---------+----------------------+-----+\n",
            "|len_hashtags|len_domains|len_links|present_media_language|label|\n",
            "+------------+-----------+---------+----------------------+-----+\n",
            "|         0.6|        0.9|      0.0|                   534|    1|\n",
            "|        0.94|        0.9|      0.6|                   349|    1|\n",
            "|        0.48|        0.0|      0.3|                   413|    1|\n",
            "|        0.68|        0.6|      1.0|                    64|    0|\n",
            "|        0.66|        0.9|      0.4|                    16|    0|\n",
            "+------------+-----------+---------+----------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wide & Deep Model Training"
      ],
      "metadata": {
        "id": "R3Aq5XkR9-tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from bigdl.orca.learn.tf2.estimator import Estimator"
      ],
      "metadata": {
        "id": "f77qR5q_wxAU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wide_cols = ['engaged_with_user_is_verified', 'enaging_user_is_verified']\n",
        "wide_dims = [1, 1]\n",
        "cross_cols = ['present_media_language']\n",
        "cross_dims = [600]\n",
        "\n",
        "embedding_cols = []\n",
        "embedding_dims = []\n",
        "\n",
        "cat_cols = ['present_media',\n",
        "            'tweet_type',\n",
        "            'language']\n",
        "cat_dims = [12, 2, 66]\n",
        "count_cols = ['engaged_with_user_follower_count',\n",
        "              'engaged_with_user_following_count',\n",
        "              'enaging_user_follower_count',\n",
        "              'enaging_user_following_count']\n",
        "count_dims = [7, 7, 7, 7]\n",
        "indicator_cols = cat_cols + count_cols\n",
        "indicator_dims = cat_dims + count_dims\n",
        "\n",
        "continuous_cols = ['len_hashtags',\n",
        "                   'len_domains',\n",
        "                   'len_links']\n",
        "\n",
        "column_info = { \"wide_base_cols\": wide_cols,\n",
        "                \"wide_base_dims\": wide_dims,\n",
        "                \"wide_cross_cols\": cross_cols,\n",
        "                \"wide_cross_dims\": cross_dims,\n",
        "                \"indicator_cols\": indicator_cols,\n",
        "                \"indicator_dims\": indicator_dims,\n",
        "                \"continuous_cols\": continuous_cols,\n",
        "                \"embed_cols\": [],\n",
        "                \"embed_in_dims\": [],\n",
        "                \"embed_out_dims\": [],\n",
        "                \"label\": \"label\"}"
      ],
      "metadata": {
        "id": "qwJPSp5ByR8t"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(column_info, hidden_units=[100, 50, 25]):\n",
        "    \"\"\"Build an estimator appropriate for the given model type.\"\"\"\n",
        "    wide_base_input_layers = []\n",
        "    wide_base_layers = []\n",
        "    for i in range(len(column_info[\"wide_base_cols\"])):\n",
        "        wide_base_input_layers.append(tf.keras.layers.Input(shape=[], dtype=\"int32\"))\n",
        "        wide_base_layers.append(tf.keras.backend.one_hot(wide_base_input_layers[i], column_info[\"wide_base_dims\"][i] + 1))\n",
        "\n",
        "    wide_cross_input_layers = []\n",
        "    wide_cross_layers = []\n",
        "    for i in range(len(column_info[\"wide_cross_cols\"])):\n",
        "        wide_cross_input_layers.append(tf.keras.layers.Input(shape=[], dtype=\"int32\"))\n",
        "        wide_cross_layers.append(tf.keras.backend.one_hot(wide_cross_input_layers[i], column_info[\"wide_cross_dims\"][i]))\n",
        "\n",
        "    indicator_input_layers = []\n",
        "    indicator_layers = []\n",
        "    for i in range(len(column_info[\"indicator_cols\"])):\n",
        "        indicator_input_layers.append(tf.keras.layers.Input(shape=[], dtype=\"int32\"))\n",
        "        indicator_layers.append(tf.keras.backend.one_hot(indicator_input_layers[i], column_info[\"indicator_dims\"][i] + 1))\n",
        "\n",
        "    embed_input_layers = []\n",
        "    embed_layers = []\n",
        "    for i in range(len(column_info[\"embed_in_dims\"])):\n",
        "        embed_input_layers.append(tf.keras.layers.Input(shape=[], dtype=\"int32\"))\n",
        "        iembed = tf.keras.layers.Embedding(column_info[\"embed_in_dims\"][i] + 1,\n",
        "                                           output_dim=column_info[\"embed_out_dims\"][i])(embed_input_layers[i])\n",
        "        flat_embed = tf.keras.layers.Flatten()(iembed)\n",
        "        embed_layers.append(flat_embed)\n",
        "\n",
        "    continuous_input_layers = []\n",
        "    continuous_layers = []\n",
        "    for i in range(len(column_info[\"continuous_cols\"])):\n",
        "        continuous_input_layers.append(tf.keras.layers.Input(shape=[]))\n",
        "        continuous_layers.append(tf.keras.layers.Reshape(target_shape=(1,))(continuous_input_layers[i]))\n",
        "\n",
        "    if len(wide_base_layers + wide_cross_layers) > 1:\n",
        "        wide_input = tf.keras.layers.concatenate(wide_base_layers + wide_cross_layers, axis=1)\n",
        "    else:\n",
        "        wide_input = (wide_base_layers + wide_cross_layers)[0]\n",
        "    wide_out = tf.keras.layers.Dense(1)(wide_input)\n",
        "    if len(indicator_layers + embed_layers + continuous_layers) > 1:\n",
        "        deep_concat = tf.keras.layers.concatenate(indicator_layers +\n",
        "                                                  embed_layers +\n",
        "                                                  continuous_layers, axis=1)\n",
        "    else:\n",
        "        deep_concat = (indicator_layers + embed_layers + continuous_layers)[0]\n",
        "    linear = deep_concat\n",
        "    for ilayer in range(0, len(hidden_units)):\n",
        "        linear_mid = tf.keras.layers.Dense(hidden_units[ilayer])(linear)\n",
        "        bn = tf.keras.layers.BatchNormalization()(linear_mid)\n",
        "        relu = tf.keras.layers.ReLU()(bn)\n",
        "        dropout = tf.keras.layers.Dropout(0.1)(relu)\n",
        "        linear = dropout\n",
        "    deep_out = tf.keras.layers.Dense(1)(linear)\n",
        "    added = tf.keras.layers.add([wide_out, deep_out])\n",
        "    out = tf.keras.layers.Activation(\"sigmoid\")(added)\n",
        "    model = tf.keras.models.Model(wide_base_input_layers +\n",
        "                                  wide_cross_input_layers +\n",
        "                                  indicator_input_layers +\n",
        "                                  embed_input_layers +\n",
        "                                  continuous_input_layers,\n",
        "                                  out)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sXH7Ys4ZyVY4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"lr\": 0.0001,\n",
        "    \"column_info\": column_info,\n",
        "    \"inter_op_parallelism\": 4,\n",
        "    \"intra_op_parallelism\": 24\n",
        "}\n",
        "batch_size = 2560"
      ],
      "metadata": {
        "id": "lIzRRgGgymJP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_creator(config):\n",
        "    model = build_model(column_info=config[\"column_info\"],\n",
        "                        hidden_units=[1024, 1024])\n",
        "    optimizer = tf.keras.optimizers.Adam(config[\"lr\"])\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['binary_accuracy', 'binary_crossentropy', 'AUC', 'Precision', 'Recall'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "JuhASNfdy0Op"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = Estimator.from_keras(\n",
        "    model_creator=model_creator,\n",
        "    verbose=True,\n",
        "    config=config,\n",
        "    workers_per_node=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W48FAO-Qy3w8",
        "outputId": "c6882eb1-d195-429b-93fd-bec3f8f36172"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bigdl/orca/learn/tf2/tf_runner.py:318: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bigdl/orca/learn/tf2/tf_runner.py:318: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n",
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m 2022-08-22 10:14:53.802931: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m 2022-08-22 10:14:53.810244: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = math.ceil(train_size / batch_size)\n",
        "epochs = 5\n",
        "val_steps = math.ceil(valid_size / batch_size)\n",
        "\n",
        "callbacks = [EarlyStopping(monitor='val_auc', mode='max', verbose=1, patience=3)]"
      ],
      "metadata": {
        "id": "v7ANrx510Z3C"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_cols(column_info):\n",
        "    return [column_info[\"label\"]]\n",
        "\n",
        "def feature_cols(column_info):\n",
        "    return column_info[\"wide_base_cols\"] + column_info[\"wide_cross_cols\"] +\\\n",
        "                  column_info[\"indicator_cols\"] + column_info[\"embed_cols\"] + column_info[\"continuous_cols\"]\n",
        "\n",
        "estimator.fit(data=train_tbl.df,\n",
        "              epochs=epochs,\n",
        "              batch_size=batch_size,\n",
        "              steps_per_epoch=steps_per_epoch,\n",
        "              validation_data=valid_tbl.df,\n",
        "              validation_steps=val_steps,\n",
        "              callbacks=callbacks,\n",
        "              feature_cols=feature_cols(column_info),\n",
        "              label_cols=label_cols(column_info))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJV4ml1Vy_4D",
        "outputId": "289e8407-cf55-42a0-ca1c-0a32e2154e8e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bigdl/orca/learn/tf2/tf_runner.py:196: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m rename to distribute_datasets_from_function\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/bigdl/orca/learn/tf2/tf_runner.py:196: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m Instructions for updating:\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m rename to distribute_datasets_from_function\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m 2022-08-22 10:16:22.416816: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m 2022-08-22 10:16:22.462613: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m Epoch 1/5\n",
            " 1/16 [>.............................] - ETA: 2:59 - loss: 0.5433 - binary_accuracy: 0.7414 - binary_crossentropy: 0.5433 - auc: 0.5111 - precision: 0.9361 - recall: 0.7765\n",
            " 2/16 [==>...........................] - ETA: 12s - loss: 0.5302 - binary_accuracy: 0.7588 - binary_crossentropy: 0.5302 - auc: 0.5036 - precision: 0.9342 - recall: 0.7982 \n",
            " 3/16 [====>.........................] - ETA: 10s - loss: 0.5207 - binary_accuracy: 0.7712 - binary_crossentropy: 0.5207 - auc: 0.5052 - precision: 0.9352 - recall: 0.8116\n",
            " 4/16 [======>.......................] - ETA: 9s - loss: 0.5104 - binary_accuracy: 0.7836 - binary_crossentropy: 0.5104 - auc: 0.5025 - precision: 0.9369 - recall: 0.8246 \n",
            " 5/16 [========>.....................] - ETA: 8s - loss: 0.5022 - binary_accuracy: 0.7924 - binary_crossentropy: 0.5022 - auc: 0.5038 - precision: 0.9368 - recall: 0.8348\n",
            " 6/16 [==========>...................] - ETA: 7s - loss: 0.4928 - binary_accuracy: 0.8040 - binary_crossentropy: 0.4928 - auc: 0.5089 - precision: 0.9380 - recall: 0.8470\n",
            " 7/16 [============>.................] - ETA: 6s - loss: 0.4852 - binary_accuracy: 0.8142 - binary_crossentropy: 0.4852 - auc: 0.5060 - precision: 0.9384 - recall: 0.8583\n",
            " 8/16 [==============>...............] - ETA: 6s - loss: 0.4776 - binary_accuracy: 0.8225 - binary_crossentropy: 0.4776 - auc: 0.5036 - precision: 0.9392 - recall: 0.8670\n",
            " 9/16 [===============>..............] - ETA: 5s - loss: 0.4709 - binary_accuracy: 0.8299 - binary_crossentropy: 0.4709 - auc: 0.5035 - precision: 0.9383 - recall: 0.8762\n",
            "10/16 [=================>............] - ETA: 4s - loss: 0.4642 - binary_accuracy: 0.8359 - binary_crossentropy: 0.4642 - auc: 0.5053 - precision: 0.9381 - recall: 0.8832\n",
            "11/16 [===================>..........] - ETA: 3s - loss: 0.4576 - binary_accuracy: 0.8423 - binary_crossentropy: 0.4576 - auc: 0.5083 - precision: 0.9378 - recall: 0.8908\n",
            "12/16 [=====================>........] - ETA: 3s - loss: 0.4506 - binary_accuracy: 0.8490 - binary_crossentropy: 0.4506 - auc: 0.5111 - precision: 0.9384 - recall: 0.8979\n",
            "13/16 [=======================>......] - ETA: 2s - loss: 0.4450 - binary_accuracy: 0.8543 - binary_crossentropy: 0.4450 - auc: 0.5096 - precision: 0.9382 - recall: 0.9042\n",
            "14/16 [=========================>....] - ETA: 1s - loss: 0.4401 - binary_accuracy: 0.8581 - binary_crossentropy: 0.4401 - auc: 0.5073 - precision: 0.9374 - recall: 0.9094\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.4341 - binary_accuracy: 0.8626 - binary_crossentropy: 0.4341 - auc: 0.5092 - precision: 0.9376 - recall: 0.9142\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.4289 - binary_accuracy: 0.8664 - binary_crossentropy: 0.4289 - auc: 0.5090 - precision: 0.9377 - recall: 0.9185\n",
            "16/16 [==============================] - 29s 1s/step - loss: 0.4289 - binary_accuracy: 0.8664 - binary_crossentropy: 0.4289 - auc: 0.5090 - precision: 0.9377 - recall: 0.9185 - val_loss: 0.5945 - val_binary_accuracy: 0.9360 - val_binary_crossentropy: 0.5945 - val_auc: 0.4843 - val_precision: 0.9361 - val_recall: 0.9999\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m Epoch 2/5\n",
            " 1/16 [>.............................] - ETA: 12s - loss: 0.3398 - binary_accuracy: 0.9285 - binary_crossentropy: 0.3398 - auc: 0.5040 - precision: 0.9380 - recall: 0.9892\n",
            " 2/16 [==>...........................] - ETA: 11s - loss: 0.3391 - binary_accuracy: 0.9281 - binary_crossentropy: 0.3391 - auc: 0.5034 - precision: 0.9367 - recall: 0.9902\n",
            " 3/16 [====>.........................] - ETA: 10s - loss: 0.3329 - binary_accuracy: 0.9302 - binary_crossentropy: 0.3329 - auc: 0.5187 - precision: 0.9375 - recall: 0.9917\n",
            " 4/16 [======>.......................] - ETA: 9s - loss: 0.3297 - binary_accuracy: 0.9306 - binary_crossentropy: 0.3297 - auc: 0.5217 - precision: 0.9373 - recall: 0.9923 \n",
            " 5/16 [========>.....................] - ETA: 8s - loss: 0.3292 - binary_accuracy: 0.9292 - binary_crossentropy: 0.3292 - auc: 0.5210 - precision: 0.9355 - recall: 0.9927\n",
            " 6/16 [==========>...................] - ETA: 7s - loss: 0.3271 - binary_accuracy: 0.9292 - binary_crossentropy: 0.3271 - auc: 0.5239 - precision: 0.9351 - recall: 0.9932\n",
            " 7/16 [============>.................] - ETA: 6s - loss: 0.3240 - binary_accuracy: 0.9301 - binary_crossentropy: 0.3240 - auc: 0.5228 - precision: 0.9354 - recall: 0.9940\n",
            " 8/16 [==============>...............] - ETA: 6s - loss: 0.3214 - binary_accuracy: 0.9302 - binary_crossentropy: 0.3214 - auc: 0.5239 - precision: 0.9352 - recall: 0.9943\n",
            " 9/16 [===============>..............] - ETA: 5s - loss: 0.3172 - binary_accuracy: 0.9316 - binary_crossentropy: 0.3172 - auc: 0.5252 - precision: 0.9361 - recall: 0.9949\n",
            "10/16 [=================>............] - ETA: 4s - loss: 0.3139 - binary_accuracy: 0.9322 - binary_crossentropy: 0.3139 - auc: 0.5277 - precision: 0.9364 - recall: 0.9952\n",
            "11/16 [===================>..........] - ETA: 3s - loss: 0.3118 - binary_accuracy: 0.9324 - binary_crossentropy: 0.3118 - auc: 0.5284 - precision: 0.9362 - recall: 0.9955\n",
            "12/16 [=====================>........] - ETA: 3s - loss: 0.3098 - binary_accuracy: 0.9327 - binary_crossentropy: 0.3098 - auc: 0.5257 - precision: 0.9363 - recall: 0.9959\n",
            "13/16 [=======================>......] - ETA: 2s - loss: 0.3061 - binary_accuracy: 0.9336 - binary_crossentropy: 0.3061 - auc: 0.5302 - precision: 0.9369 - recall: 0.9962\n",
            "14/16 [=========================>....] - ETA: 1s - loss: 0.3034 - binary_accuracy: 0.9340 - binary_crossentropy: 0.3034 - auc: 0.5320 - precision: 0.9371 - recall: 0.9964\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.3017 - binary_accuracy: 0.9337 - binary_crossentropy: 0.3017 - auc: 0.5347 - precision: 0.9367 - recall: 0.9965\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2993 - binary_accuracy: 0.9341 - binary_crossentropy: 0.2993 - auc: 0.5357 - precision: 0.9370 - recall: 0.9967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m 2022-08-22 10:17:04.083371: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m 2022-08-22 10:17:04.153217: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r16/16 [==============================] - 14s 850ms/step - loss: 0.2993 - binary_accuracy: 0.9341 - binary_crossentropy: 0.2993 - auc: 0.5357 - precision: 0.9370 - recall: 0.9967 - val_loss: 0.5610 - val_binary_accuracy: 0.9361 - val_binary_crossentropy: 0.5610 - val_auc: 0.4867 - val_precision: 0.9361 - val_recall: 1.0000\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m Epoch 3/5\n",
            " 1/16 [>.............................] - ETA: 11s - loss: 0.2509 - binary_accuracy: 0.9434 - binary_crossentropy: 0.2509 - auc: 0.5865 - precision: 0.9437 - recall: 0.9996\n",
            " 2/16 [==>...........................] - ETA: 11s - loss: 0.2572 - binary_accuracy: 0.9416 - binary_crossentropy: 0.2572 - auc: 0.5480 - precision: 0.9420 - recall: 0.9996\n",
            " 3/16 [====>.........................] - ETA: 10s - loss: 0.2594 - binary_accuracy: 0.9397 - binary_crossentropy: 0.2594 - auc: 0.5521 - precision: 0.9401 - recall: 0.9996\n",
            " 4/16 [======>.......................] - ETA: 9s - loss: 0.2608 - binary_accuracy: 0.9380 - binary_crossentropy: 0.2608 - auc: 0.5584 - precision: 0.9383 - recall: 0.9997 \n",
            " 5/16 [========>.....................] - ETA: 8s - loss: 0.2612 - binary_accuracy: 0.9375 - binary_crossentropy: 0.2612 - auc: 0.5539 - precision: 0.9378 - recall: 0.9997\n",
            " 6/16 [==========>...................] - ETA: 7s - loss: 0.2615 - binary_accuracy: 0.9370 - binary_crossentropy: 0.2615 - auc: 0.5490 - precision: 0.9373 - recall: 0.9997\n",
            " 7/16 [============>.................] - ETA: 7s - loss: 0.2602 - binary_accuracy: 0.9374 - binary_crossentropy: 0.2602 - auc: 0.5450 - precision: 0.9377 - recall: 0.9997\n",
            " 8/16 [==============>...............] - ETA: 6s - loss: 0.2602 - binary_accuracy: 0.9370 - binary_crossentropy: 0.2602 - auc: 0.5452 - precision: 0.9372 - recall: 0.9997\n",
            " 9/16 [===============>..............] - ETA: 5s - loss: 0.2594 - binary_accuracy: 0.9368 - binary_crossentropy: 0.2594 - auc: 0.5478 - precision: 0.9370 - recall: 0.9998\n",
            "10/16 [=================>............] - ETA: 4s - loss: 0.2583 - binary_accuracy: 0.9369 - binary_crossentropy: 0.2583 - auc: 0.5491 - precision: 0.9371 - recall: 0.9998\n",
            "11/16 [===================>..........] - ETA: 3s - loss: 0.2569 - binary_accuracy: 0.9370 - binary_crossentropy: 0.2569 - auc: 0.5534 - precision: 0.9372 - recall: 0.9998\n",
            "12/16 [=====================>........] - ETA: 3s - loss: 0.2564 - binary_accuracy: 0.9367 - binary_crossentropy: 0.2564 - auc: 0.5579 - precision: 0.9368 - recall: 0.9998\n",
            "13/16 [=======================>......] - ETA: 2s - loss: 0.2560 - binary_accuracy: 0.9365 - binary_crossentropy: 0.2560 - auc: 0.5591 - precision: 0.9366 - recall: 0.9998\n",
            "14/16 [=========================>....] - ETA: 1s - loss: 0.2565 - binary_accuracy: 0.9359 - binary_crossentropy: 0.2565 - auc: 0.5598 - precision: 0.9361 - recall: 0.9998\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.2562 - binary_accuracy: 0.9359 - binary_crossentropy: 0.2562 - auc: 0.5581 - precision: 0.9360 - recall: 0.9998\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2542 - binary_accuracy: 0.9365 - binary_crossentropy: 0.2542 - auc: 0.5605 - precision: 0.9366 - recall: 0.9998\n",
            "16/16 [==============================] - 14s 874ms/step - loss: 0.2542 - binary_accuracy: 0.9365 - binary_crossentropy: 0.2542 - auc: 0.5605 - precision: 0.9366 - recall: 0.9998 - val_loss: 0.5103 - val_binary_accuracy: 0.9361 - val_binary_crossentropy: 0.5103 - val_auc: 0.4863 - val_precision: 0.9361 - val_recall: 1.0000\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m Epoch 4/5\n",
            " 1/16 [>.............................] - ETA: 11s - loss: 0.2570 - binary_accuracy: 0.9289 - binary_crossentropy: 0.2570 - auc: 0.6123 - precision: 0.9289 - recall: 1.0000\n",
            " 2/16 [==>...........................] - ETA: 11s - loss: 0.2536 - binary_accuracy: 0.9307 - binary_crossentropy: 0.2536 - auc: 0.6037 - precision: 0.9307 - recall: 1.0000\n",
            " 3/16 [====>.........................] - ETA: 10s - loss: 0.2552 - binary_accuracy: 0.9297 - binary_crossentropy: 0.2552 - auc: 0.6033 - precision: 0.9297 - recall: 1.0000\n",
            " 4/16 [======>.......................] - ETA: 9s - loss: 0.2485 - binary_accuracy: 0.9328 - binary_crossentropy: 0.2485 - auc: 0.6038 - precision: 0.9328 - recall: 1.0000 \n",
            " 5/16 [========>.....................] - ETA: 8s - loss: 0.2456 - binary_accuracy: 0.9340 - binary_crossentropy: 0.2456 - auc: 0.6031 - precision: 0.9340 - recall: 1.0000\n",
            " 6/16 [==========>...................] - ETA: 7s - loss: 0.2462 - binary_accuracy: 0.9341 - binary_crossentropy: 0.2462 - auc: 0.5936 - precision: 0.9341 - recall: 1.0000\n",
            " 7/16 [============>.................] - ETA: 6s - loss: 0.2434 - binary_accuracy: 0.9355 - binary_crossentropy: 0.2434 - auc: 0.5920 - precision: 0.9355 - recall: 1.0000\n",
            " 8/16 [==============>...............] - ETA: 6s - loss: 0.2406 - binary_accuracy: 0.9363 - binary_crossentropy: 0.2406 - auc: 0.5972 - precision: 0.9363 - recall: 0.9999\n",
            " 9/16 [===============>..............] - ETA: 5s - loss: 0.2379 - binary_accuracy: 0.9374 - binary_crossentropy: 0.2379 - auc: 0.5967 - precision: 0.9375 - recall: 1.0000\n",
            "10/16 [=================>............] - ETA: 4s - loss: 0.2380 - binary_accuracy: 0.9375 - binary_crossentropy: 0.2380 - auc: 0.5941 - precision: 0.9375 - recall: 1.0000\n",
            "11/16 [===================>..........] - ETA: 3s - loss: 0.2376 - binary_accuracy: 0.9377 - binary_crossentropy: 0.2376 - auc: 0.5909 - precision: 0.9377 - recall: 1.0000\n",
            "12/16 [=====================>........] - ETA: 2s - loss: 0.2379 - binary_accuracy: 0.9373 - binary_crossentropy: 0.2379 - auc: 0.5934 - precision: 0.9374 - recall: 0.9999\n",
            "13/16 [=======================>......] - ETA: 2s - loss: 0.2376 - binary_accuracy: 0.9373 - binary_crossentropy: 0.2376 - auc: 0.5953 - precision: 0.9373 - recall: 0.9999\n",
            "14/16 [=========================>....] - ETA: 1s - loss: 0.2376 - binary_accuracy: 0.9372 - binary_crossentropy: 0.2376 - auc: 0.5950 - precision: 0.9372 - recall: 0.9999\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.2369 - binary_accuracy: 0.9373 - binary_crossentropy: 0.2369 - auc: 0.5969 - precision: 0.9374 - recall: 0.9999\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2374 - binary_accuracy: 0.9369 - binary_crossentropy: 0.2374 - auc: 0.5986 - precision: 0.9370 - recall: 0.9999\n",
            "16/16 [==============================] - 13s 823ms/step - loss: 0.2374 - binary_accuracy: 0.9369 - binary_crossentropy: 0.2374 - auc: 0.5986 - precision: 0.9370 - recall: 0.9999 - val_loss: 0.4691 - val_binary_accuracy: 0.9361 - val_binary_crossentropy: 0.4691 - val_auc: 0.4892 - val_precision: 0.9361 - val_recall: 1.0000\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m Epoch 5/5\n",
            " 1/16 [>.............................] - ETA: 10s - loss: 0.2228 - binary_accuracy: 0.9418 - binary_crossentropy: 0.2228 - auc: 0.6043 - precision: 0.9418 - recall: 1.0000\n",
            " 2/16 [==>...........................] - ETA: 9s - loss: 0.2341 - binary_accuracy: 0.9379 - binary_crossentropy: 0.2341 - auc: 0.5851 - precision: 0.9379 - recall: 1.0000 \n",
            " 3/16 [====>.........................] - ETA: 8s - loss: 0.2266 - binary_accuracy: 0.9405 - binary_crossentropy: 0.2266 - auc: 0.5949 - precision: 0.9405 - recall: 1.0000\n",
            " 4/16 [======>.......................] - ETA: 8s - loss: 0.2376 - binary_accuracy: 0.9363 - binary_crossentropy: 0.2376 - auc: 0.5900 - precision: 0.9363 - recall: 1.0000\n",
            " 5/16 [========>.....................] - ETA: 7s - loss: 0.2368 - binary_accuracy: 0.9365 - binary_crossentropy: 0.2368 - auc: 0.5933 - precision: 0.9365 - recall: 1.0000\n",
            " 6/16 [==========>...................] - ETA: 6s - loss: 0.2380 - binary_accuracy: 0.9360 - binary_crossentropy: 0.2380 - auc: 0.5919 - precision: 0.9360 - recall: 1.0000\n",
            " 7/16 [============>.................] - ETA: 5s - loss: 0.2366 - binary_accuracy: 0.9365 - binary_crossentropy: 0.2366 - auc: 0.5923 - precision: 0.9365 - recall: 1.0000\n",
            " 8/16 [==============>...............] - ETA: 5s - loss: 0.2366 - binary_accuracy: 0.9364 - binary_crossentropy: 0.2366 - auc: 0.5952 - precision: 0.9364 - recall: 1.0000\n",
            " 9/16 [===============>..............] - ETA: 4s - loss: 0.2356 - binary_accuracy: 0.9365 - binary_crossentropy: 0.2356 - auc: 0.6000 - precision: 0.9365 - recall: 1.0000\n",
            "10/16 [=================>............] - ETA: 3s - loss: 0.2366 - binary_accuracy: 0.9361 - binary_crossentropy: 0.2366 - auc: 0.6005 - precision: 0.9361 - recall: 1.0000\n",
            "11/16 [===================>..........] - ETA: 3s - loss: 0.2371 - binary_accuracy: 0.9359 - binary_crossentropy: 0.2371 - auc: 0.5983 - precision: 0.9360 - recall: 1.0000\n",
            "12/16 [=====================>........] - ETA: 2s - loss: 0.2370 - binary_accuracy: 0.9356 - binary_crossentropy: 0.2370 - auc: 0.6045 - precision: 0.9356 - recall: 1.0000\n",
            "13/16 [=======================>......] - ETA: 2s - loss: 0.2363 - binary_accuracy: 0.9359 - binary_crossentropy: 0.2363 - auc: 0.6039 - precision: 0.9360 - recall: 1.0000\n",
            "14/16 [=========================>....] - ETA: 1s - loss: 0.2360 - binary_accuracy: 0.9361 - binary_crossentropy: 0.2360 - auc: 0.6025 - precision: 0.9362 - recall: 1.0000\n",
            "15/16 [===========================>..] - ETA: 0s - loss: 0.2348 - binary_accuracy: 0.9364 - binary_crossentropy: 0.2348 - auc: 0.6054 - precision: 0.9365 - recall: 0.9999\n",
            "16/16 [==============================] - ETA: 0s - loss: 0.2351 - binary_accuracy: 0.9362 - binary_crossentropy: 0.2351 - auc: 0.6051 - precision: 0.9363 - recall: 0.9999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[2m\u001b[36m(Worker pid=485)\u001b[0m 2022-08-22 10:17:43.379109: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m 2022-08-22 10:17:43.369918: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2m\u001b[36m(Worker pid=486)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r16/16 [==============================] - 12s 775ms/step - loss: 0.2351 - binary_accuracy: 0.9362 - binary_crossentropy: 0.2351 - auc: 0.6051 - precision: 0.9363 - recall: 0.9999 - val_loss: 0.4321 - val_binary_accuracy: 0.9361 - val_binary_crossentropy: 0.4321 - val_auc: 0.4918 - val_precision: 0.9361 - val_recall: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'train_loss': 0.23513731360435486,\n",
              "  'train_binary_accuracy': 0.936230480670929,\n",
              "  'train_binary_crossentropy': 0.23513731360435486,\n",
              "  'train_auc': 0.6051348447799683,\n",
              "  'train_precision': 0.9362990260124207,\n",
              "  'train_recall': 0.9999217987060547,\n",
              "  'train_val_loss': 0.432126522064209,\n",
              "  'train_val_binary_accuracy': 0.9361327886581421,\n",
              "  'train_val_binary_crossentropy': 0.432126522064209,\n",
              "  'train_val_auc': 0.4918055534362793,\n",
              "  'train_val_precision': 0.9361327886581421,\n",
              "  'train_val_recall': 1.0}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = estimator.get_model()\n",
        "tf.saved_model.save(model, \"recsys_wnd/\")\n",
        "\n",
        "stop_orca_context()"
      ],
      "metadata": {
        "id": "9vbE___M03UJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8e4536-9fa7-49f4-a5fa-6364a2d6d9db"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopping orca context\n"
          ]
        }
      ]
    }
  ]
}